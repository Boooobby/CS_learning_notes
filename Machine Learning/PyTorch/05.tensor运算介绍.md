好的！我来详细讲解 PyTorch 张量的运算。张量运算可以分为几个主要类别：

## 1. 基本算术运算

### 逐元素运算
```python
import torch

x = torch.tensor([1, 2, 3.])
y = torch.tensor([4, 5, 6.])

# 加法
print(x + y)        # tensor([5., 7., 9.])
print(torch.add(x, y))  # 等价写法

# 减法
print(x - y)        # tensor([-3., -3., -3.])

# 乘法（逐元素）
print(x * y)        # tensor([4., 10., 18.])
print(torch.mul(x, y))  # 等价写法

# 除法（逐元素）
print(x / y)        # tensor([0.2500, 0.4000, 0.5000])
print(torch.div(x, y))  # 等价写法

# 幂运算
print(x ** 2)       # tensor([1., 4., 9.])
print(torch.pow(x, 2))  # 等价写法
```

### 原地操作（In-place Operations）
```python
x = torch.tensor([1, 2, 3.])
y = torch.tensor([4, 5, 6.])

# 非原地操作 - 创建新张量
z = x + y
print(x)  # 不变: tensor([1., 2., 3.])

# 原地操作 - 修改原始张量
x.add_(y)  # 注意下划线后缀
print(x)  # 改变: tensor([5., 7., 9.])
```

## 2. 矩阵运算

### 矩阵乘法
```python
# 2D 张量（矩阵）
A = torch.tensor([[1, 2], [3, 4.]])
B = torch.tensor([[5, 6], [7, 8.]])

# 矩阵乘法
print(A @ B)           # tensor([[19., 22.], [43., 50.]])
print(torch.matmul(A, B))  # 等价写法
print(A.mm(B))         # 专门用于2D矩阵乘法

# 点积（1D 张量）
v1 = torch.tensor([1, 2, 3.])
v2 = torch.tensor([4, 5, 6.])
print(torch.dot(v1, v2))  # tensor(32.)
```

### 转置和维度变换
```python
A = torch.tensor([[1, 2, 3], [4, 5, 6.]])
print(A.shape)  # torch.Size([2, 3])

# 转置
print(A.T)      # tensor([[1., 4.], [2., 5.], [3., 6.]])
print(A.t())    # 等价写法

# 高维张量转置
B = torch.randn(2, 3, 4)
print(B.permute(2, 0, 1).shape)  # torch.Size([4, 2, 3])
```

## 3. 广播机制（Broadcasting）

PyTorch 自动处理不同形状张量间的运算：

```python
# 标量与张量
A = torch.tensor([[1, 2], [3, 4.]])
print(A + 1)    # tensor([[2., 3.], [4., 5.]])

# 不同形状张量
B = torch.tensor([10, 20.])  # shape: (2,)
print(A + B)    # tensor([[11., 22.], [13., 24.]])

# 广播规则：从尾部维度开始匹配
C = torch.randn(3, 1, 4)
D = torch.randn(2, 4)
# C: (3, 1, 4)
# D:    (2, 4)
# 结果: (3, 2, 4)
```

## 4. 归约运算（Reduction Operations）

### 统计运算
```python
x = torch.tensor([[1, 2, 3], [4, 5, 6.]], dtype=torch.float)

# 求和
print(x.sum())           # 所有元素和: tensor(21.)
print(x.sum(dim=0))      # 沿维度0求和: tensor([5., 7., 9.])
print(x.sum(dim=1))      # 沿维度1求和: tensor([6., 15.])

# 均值
print(x.mean())          # 所有元素均值: tensor(3.5000)
print(x.mean(dim=0))     # 沿维度0求均值: tensor([2.5000, 3.5000, 4.5000])

# 最大值/最小值
print(x.max())           # 最大值: tensor(6.)
print(x.min(dim=1))      # 沿维度1求最小值，返回(values, indices)
# (tensor([1., 4.]), tensor([0, 0]))

# 其他统计量
print(x.std())           # 标准差
print(x.var())           # 方差
print(x.prod())          # 所有元素乘积
```

## 5. 比较运算

```python
x = torch.tensor([1, 2, 3, 4, 5.])
y = torch.tensor([3, 3, 3, 3, 3.])

# 元素比较
print(x > y)        # tensor([False, False, False,  True,  True])
print(x == y)       # tensor([False, False,  True, False, False])
print(x >= 3)       # tensor([False, False,  True,  True,  True])

# 聚合比较
print(torch.all(x > 0))    # 是否所有元素都满足: tensor(True)
print(torch.any(x < 0))    # 是否有任一元素满足: tensor(False)

# 使用掩码选择元素
mask = x > 2
print(mask)         # tensor([False, False,  True,  True,  True])
print(x[mask])      # tensor([3., 4., 5.]) - 只选择True位置的元素
```

## 6. 三角函数和指数运算

```python
x = torch.tensor([0., torch.pi/4, torch.pi/2])

# 三角函数
print(torch.sin(x))     # 正弦
print(torch.cos(x))     # 余弦
print(torch.tan(x))     # 正切

# 指数和对数
y = torch.tensor([1., 2., 3.])
print(torch.exp(y))     # e^y: tensor([ 2.7183,  7.3891, 20.0855])
print(torch.log(y))     # 自然对数
print(torch.log10(y))   # 以10为底的对数

# 舍入运算
z = torch.tensor([1.4, 1.5, 1.6, -1.4, -1.5])
print(torch.round(z))   # 四舍五入
print(torch.floor(z))   # 向下取整
print(torch.ceil(z))    # 向上取整
```

## 7. 实际应用示例

### 神经网络前向传播
```python
# 模拟一个简单的全连接层
def linear_layer(inputs, weights, bias):
    return inputs @ weights.T + bias  # 矩阵乘法 + 广播加法

# 数据
inputs = torch.randn(10, 5)      # 10个样本，每个5维特征
weights = torch.randn(3, 5)      # 3个神经元，每个5维权重
bias = torch.randn(3)           # 3个偏置

# 前向传播
output = linear_layer(inputs, weights, bias)
print(output.shape)  # torch.Size([10, 3])
```

### 数据标准化
```python
# 对每个特征进行标准化
data = torch.randn(100, 5)  # 100个样本，5个特征

# 计算每个特征的均值和标准差
mean = data.mean(dim=0)     # 沿样本维度求均值
std = data.std(dim=0)       # 沿样本维度求标准差

# 标准化
normalized_data = (data - mean) / std
print(f"标准化后均值: {normalized_data.mean(dim=0)}")
print(f"标准化后方差: {normalized_data.var(dim=0)}")
```

## 8. 性能优化技巧

### 使用向量化操作
```python
# 不好的写法 - 使用循环
result = torch.zeros(1000)
for i in range(1000):
    result[i] = i ** 2

# 好的写法 - 向量化操作
result = torch.arange(1000, dtype=torch.float) ** 2
```

### 避免不必要的拷贝
```python
# 使用原地操作减少内存分配
x = torch.randn(1000, 1000)
# 不好的写法
x = x * 2 + 1

# 好的写法
x.mul_(2).add_(1)
```

## 总结

PyTorch 张量运算的主要特点：

1. **丰富的运算类型**：算术、矩阵、统计、比较等
2. **广播机制**：自动处理不同形状张量的运算
3. **原地操作**：通过 `_` 后缀节省内存
4. **GPU 加速**：所有运算都支持 GPU 加速
5. **自动求导**：所有运算都支持梯度计算

掌握这些运算是使用 PyTorch 进行深度学习的基础！