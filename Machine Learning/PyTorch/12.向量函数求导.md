太好了！这张图片正好解释了 PyTorch 自动求导的数学原理。让我为你补充向量函数微分的相关知识。

## 1. 从标量函数到向量函数

### 标量函数（我们已经熟悉的）
```python
# f(x) = x²，输入标量，输出标量
x = torch.tensor(2.0, requires_grad=True)
y = x**2
y.backward()  # dy/dx = 2x = 4.0
```

### 向量函数（新的概念）
```python
# f(x) = [x², x³]，输入标量，输出向量
x = torch.tensor(2.0, requires_grad=True)
y = torch.stack([x**2, x**3])  # y = [4, 8]
```

**问题**：如何求导？答案是：我们得到的是一个**向量**的导数！

## 2. 雅可比矩阵（Jacobian Matrix）

### 定义
对于向量函数 \(\vec{y} = f(\vec{x})\)，其中：
- \(\vec{x} = [x_1, x_2, ..., x_n]\) 是输入向量
- \(\vec{y} = [y_1, y_2, ..., y_m]\) 是输出向量

雅可比矩阵 \(J\) 包含所有偏导数：

\[
J = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial x_1} & \frac{\partial y_m}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
\]

### 具体例子
```python
# 例子：y = [x₁² + x₂, x₁ + x₂³]
# 输入：x = [x₁, x₂]，输出：y = [y₁, y₂]

# 雅可比矩阵：
# J = [[∂y₁/∂x₁, ∂y₁/∂x₂],
#      [∂y₂/∂x₁, ∂y₂/∂x₂]]
#   = [[2x₁, 1],
#      [1, 3x₂²]]
```

## 3. 向量-雅可比乘积（Vector-Jacobian Product）

### 核心思想
PyTorch 不直接计算完整的雅可比矩阵（可能很大），而是计算**向量-雅可比乘积**：

\[
J^T \cdot \vec{v}
\]

其中 \(\vec{v}\) 是一个与输出 \(\vec{y}\) 同维度的向量。

### 数学推导
如果 \(l = g(\vec{y})\) 是一个标量函数，那么：

\[
\frac{\partial l}{\partial \vec{x}} = J^T \cdot \frac{\partial l}{\partial \vec{y}}
\]

展开就是：

\[
\begin{bmatrix}
\frac{\partial l}{\partial x_1} \\
\frac{\partial l}{\partial x_2} \\
\vdots \\
\frac{\partial l}{\partial x_n}
\end{bmatrix}
=
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_2}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_1} \\
\frac{\partial y_1}{\partial x_2} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_m}{\partial x_2} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial y_1}{\partial x_n} & \frac{\partial y_2}{\partial x_n} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
\begin{bmatrix}
\frac{\partial l}{\partial y_1} \\
\frac{\partial l}{\partial y_2} \\
\vdots \\
\frac{\partial l}{\partial y_m}
\end{bmatrix}
\]

## 4. 在 PyTorch 中的具体应用

### 情况1：输出是标量（最常见）
```python
# 神经网络损失函数通常是标量
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = x[0]**2 + x[1]**3  # 标量输出
loss = y
loss.backward()  # v = [1] (隐含的)
```

### 情况2：输出是向量
```python
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = torch.stack([x[0]**2, x[1]**3])  # 向量输出 [1, 8]

# 方法1：先变成标量
y.sum().backward()  # v = [1, 1]

# 方法2：直接指定 v
v = torch.tensor([0.5, 2.0])  # 自定义权重
y.backward(gradient=v)  # v = [0.5, 2.0]
```

## 5. 实际计算示例

让我们手动验证一下：

```python
import torch

# 定义函数：y = [x₁², x₂³]
x = torch.tensor([2.0, 3.0], requires_grad=True)
y = torch.stack([x[0]**2, x[1]**3])  # y = [4, 27]

# 理论雅可比矩阵：
# J = [[2x₁, 0],
#      [0, 3x₂²]] = [[4, 0],
#                    [0, 27]]

# 使用不同的 v 进行测试
def test_with_v(v):
    x.grad = None  # 清除梯度
    y.backward(gradient=torch.tensor(v))
    print(f"v = {v}, x.grad = {x.grad}")

test_with_v([1, 1])    # [4×1 + 0×1, 0×1 + 27×1] = [4, 27]
test_with_v([2, 0])    # [4×2 + 0×0, 0×2 + 27×0] = [8, 0]  
test_with_v([0, 0.5])  # [4×0 + 0×0.5, 0×0 + 27×0.5] = [0, 13.5]
```

## 6. 在神经网络中的意义

### 损失函数场景
```python
# 在多任务学习中很常见
x = torch.tensor([1.0, 2.0], requires_grad=True)

# 多个输出头（比如同时做分类和回归）
classification_output = x[0]**2      # 分类损失相关
regression_output = x[1]**3          # 回归损失相关
multi_output = torch.stack([classification_output, regression_output])

# 给不同任务不同的权重
task_weights = torch.tensor([0.7, 0.3])  # 更重视分类任务
multi_output.backward(gradient=task_weights)
```

## 7. 记忆技巧

**简单理解**：
- `external_grad` 或 `gradient` 参数就是向量 \(\vec{v}\)
- 它代表了**最终标量损失对每个输出分量的敏感度**
- PyTorch 通过 \(J^T \cdot \vec{v}\) 高效计算梯度

**类比**：
想象你有多个收入来源（输出），`external_grad` 告诉系统你对每个收入来源的重视程度。

## 总结

向量函数微分的核心：
1. **雅可比矩阵**：描述向量函数的所有一阶偏导数
2. **向量-雅可比乘积**：PyTorch 的高效计算方式
3. **`external_grad`**：指定最终损失对各个输出的依赖程度
4. **链式法则**：将向量输出连接到标量损失

这就是 PyTorch 能够高效处理复杂神经网络梯度计算的数学基础！