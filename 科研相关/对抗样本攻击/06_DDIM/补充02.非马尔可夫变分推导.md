这是一个非常硬核的数学推导问题，也是理解 DDIM 论文（*Denoising Diffusion Implicit Models*）最精彩的部分。

你已经掌握了 DDPM，知道可以通过最大化 ELBO 来训练模型。DDIM 的核心贡献在于它发现了一个 **“巨大的漏洞”**（或者说自由度）：

**只要训练好的模型能够预测噪声 $\epsilon_\theta(x_t, t)$，那么其实并不只有 DDPM 定义的那一种前向过程（$q$）能使得这个 Loss 成立。**

下面我为你拆解这个“非马尔可夫前向过程”是如何通过变分推导构建出来的。

---

### 第一步：寻找不变量（The Invariant）

DDPM 的优化目标是：
$$ L = \mathbb{E}[\sum_{t=1}^T ||\epsilon - \epsilon_\theta(x_t, t)||^2] $$

**关键洞察：**
这个 Loss 只依赖于边缘分布（Marginals）**$q(x_t|x_0)$**。
只要 $q(x_t|x_0)$ 依然满足高斯分布 $\mathcal{N}(\sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)$，那么无论是 DDPM 还是 DDIM，它们的训练目标都是**完全一样**的。这意味着你可以直接用预训练好的 DDPM 模型，不需要重新训练。

**DDIM 的出发点**：保持 $q(x_t|x_0)$ 不变，但是修改 $x_t$ 和 $x_{t-1}$ 之间的**联合分布** $q(x_{t-1}|x_t, x_0)$。

---

### 第二步：定义非马尔可夫推理分布（Inference Distribution）

在 DDPM 中，按照贝叶斯公式，后验分布 $q(x_{t-1}|x_t, x_0)$ 是算出来的，结果是固定的。
但在 DDIM 中，作者**显式地构造**了一族新的后验分布 $q_\sigma(x_{t-1}|x_t, x_0)$。

我们需要这个分布满足两个条件：
1.  **一致性**：必须保证从这个分布出发，积分积回去后，边缘分布 $q(x_t|x_0)$ 还是原来的样子。
2.  **不确定性可控**：引入一个方差参数 $\sigma_t$ 来控制随机性。

DDIM 论文提出这样的构造（假设都是高斯分布）：

$$
q_\sigma(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_{t-1}; \underbrace{\mathbf{\mu}_\theta}_{\text{均值}}, \underbrace{\sigma_t^2 \mathbf{I}}_{\text{方差}})
$$

现在的任务是：**求出这个均值 $\mathbf{\mu}_\theta$ 应该是多少？**

#### 推导过程：
我们知道 $x_t$ 和 $x_{t-1}$ 的定义：
1.  $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon_t$
2.  $x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_{t-1}$

我们可以把 $x_{t-1}$ 理解为：一部分来自 $x_0$（信号），另一部分来自噪声。
为了构造 $q_\sigma(x_{t-1}|x_t, x_0)$，DDIM 很巧妙地利用了**正交分解**的思想。它假设 $x_{t-1}$ 的生成过程如下：

$$ x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \underbrace{\sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \cdot \frac{\mathbf{x}_t - \sqrt{\bar{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\bar{\alpha}_t}}}_{\text{确定性的噪声方向}} + \underbrace{\sigma_t \cdot \epsilon}_{\text{随机噪声}} $$

**这个公式是怎么凑出来的？**
*   **第一项**：$\sqrt{\bar{\alpha}_{t-1}} x_0$，这是为了满足 $x_{t-1}$ 时刻信号强度的定义。
*   **第三项**：$\sigma_t \epsilon$，这是我们人为引入的随机性，方差为 $\sigma_t^2$。
*   **第二项**：这是最关键的。因为总方差要是 $1 - \bar{\alpha}_{t-1}$，既然第三项占了 $\sigma_t^2$，那么剩下的方差 $\sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2}$ 必须由**已知的当前噪声**来提供。
    *   当前时刻 $t$ 的噪声是什么？就是 $\frac{x_t - \sqrt{\bar{\alpha}_t}x_0}{\sqrt{1-\bar{\alpha}_t}}$。

所以，这个分布 $q_\sigma(x_{t-1}|x_t, x_0)$ 的**均值**被定义为：

$$ \mathbf{\mu} = \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \cdot \frac{\mathbf{x}_t - \sqrt{\bar{\alpha}_t}\mathbf{x}_0}{\sqrt{1-\bar{\alpha}_t}} $$

---

### 第三步：为什么这叫“非马尔可夫”？

请注意看上面的均值 $\mathbf{\mu}$ 的公式，以及分布的写法 $q(x_{t-1}|x_t, \mathbf{x_0})$。

*   在**马尔可夫**的前向过程（DDPM）中，$x_{t-1}$ 是由 $x_{t-2}$ 生成的，或者反过来推理时只依赖 $x_t$。
*   但在 DDIM 的构造中，要得到 $x_{t-1}$，我们**显式地依赖了 $x_0$**。

从概率图模型的角度看，这种依赖关系（箭头从 $x_0$ 直接指到了 $x_{t-1}$）打破了 $x_{t-1} \perp x_0 | x_t$ 的马尔可夫性质。这就是它被称为“Non-Markovian”的原因。

---

### 第四步：从推导得到生成过程（Generative Process）

以上即是推理分布 $q$。由于我们在做生成（Sampling）的时候，根本不知道 $x_0$ 是什么，所以我们必须用神经网络 $p_\theta$ 来去**匹配**这个 $q$。

**变分推导的最后一步：替换**

既然 $q_\sigma$ 的均值公式里包含 $x_0$，生成模型 $p_\theta(x_{t-1}|x_t)$ 就做了一件简单的事：
**用神经网络预测出来的 $\hat{x}_0$ 替换掉真实未知的 $x_0$。**

我们知道神经网络预测的是噪声 $\epsilon_\theta(x_t, t)$，所以预测的 $x_0$ 可以写成：
$$ \text{predicted } x_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}} $$

把这个代入刚才那个均值公式，就得到了大家熟知的 **DDIM 采样公式**：

$$ x_{t-1} = \underbrace{\sqrt{\bar{\alpha}_{t-1}} \left( \frac{x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_\theta^{(t)}}{\sqrt{\bar{\alpha}_t}} \right)}_{\text{1. 指向预测的 } x_0} + \underbrace{\sqrt{1 - \bar{\alpha}_{t-1} - \sigma_t^2} \cdot \epsilon_\theta^{(t)}}_{\text{2. 指向同方向的噪声}} + \underbrace{\sigma_t \epsilon}_{\text{3. 随机噪声}} $$

---

### 第五步：$\sigma$ 的魔术

现在，通过非马尔可夫的变分推导，我们得到了一个带有自由参数 $\sigma_t$ 的采样公式。

1.  **当 $\sigma_t = \sqrt{\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}} \beta_t$**（从贝叶斯公式算出的标准后验方差）：
    公式完全退化回 **DDPM** 的马尔可夫采样。
2.  **当 $\sigma_t = 0$**：
    随机项消失。这意味着给定 $x_t$，下一个状态 $x_{t-1}$ 是**唯一确定**的。
    这就是 **DDIM**（确定性采样 / ODE 采样）。

### 总结：对你研究的意义

DDIM 的推导本质上是一场**数学上的“偷天换日”**：
1.  它利用了训练目标对联合分布不敏感的特性。
2.  它通过**增加这种对 $x_0$ 的直接依赖**（非马尔可夫），重新构造了 $q$ 分布。
3.  它通过调整参数 $\sigma$，把原本随机漫步的去噪过程，变成了一个**确定性的、平滑的插值过程**。

**在 DiffPure 中**，这种推导保证了你可以安全地把 SDE 换成 ODE，而不需要重新训练那个在 ImageNet 上跑了几周的 Diffusion Model，这极大地降低了科研和实验的门槛。