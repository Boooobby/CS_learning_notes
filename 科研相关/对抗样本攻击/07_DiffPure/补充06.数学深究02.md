这就到了论文的 **Section 3.2: Adaptive Attack to Diffusion Purification**。

这一部分是 DiffPure 从单纯的“防御方法”上升到“经得起推敲的科研工作”的关键。如果只提出防御而不讲如何计算梯度（或者只用数值估算梯度），在顶会上会被 Reviewer 质疑防御效果是假的（Obfuscated Gradients，梯度掩盖）。

你需要解决的核心矛盾是：**强攻击（如 PGD, AutoAttack）需要计算全链路梯度，但反向 SDE 过程极长，显存扛不住。**

我们来拆解这一节的数学，看看 DiffPure 是如何用 **Adjoint Method（伴随方法）** 在 $O(1)$ 显存下算出梯度的。

---

### 1. 问题的本质：显存爆炸 (Memory Bottleneck)

假设你正在做一次 PGD 攻击。流程如下：
1.  输入对抗样本 $x$。
2.  进入 DiffPure (SDE Solver)：$x$ 迭代 $N$ 步（比如 100 步）变成净化后的 $\hat{x}$。
3.  进入分类器：$L = Loss(\hat{x}, y)$。
4.  **反向传播求梯度**：$\nabla_x L$。

**常规 Backpropagation (BP) 的做法**：
为了计算第 0 步的梯度，你需要第 1 步的信息；为了算第 1 步，需要第 2 步……这意味着你需要把 SDE Solver 中间 $N$ 步产生的所有 $x(t)$ 全部存下来（构建计算图）。

*   显存消耗：$O(N \times \text{Image Size})$。
*   现状：一张 L20 即使有 48GB 显存，一旦 batch size 稍大或者 $N$ 变大（DiffPure 为了效果，$N$ 通常不小），直接 OOM (Out Of Memory)。

---

### 2. 解决方案：Adjoint Method (伴随方法)

DiffPure 引入了 **Neural SDE** 中的伴随方法。

**核心思想**：我们不存中间状态。既然我们知道描述变化的微分方程（SDE），那我们就在需要梯度的时候，**解一个新的微分方程，把梯度“算”出来**，而不是从缓存里“取”出来。

这对应论文中的 **Proposition 3.3** 和 **Eq. (6)**。

#### 理解扩增状态 (Augmented State)

为了同时算出“原本的图像状态”和“梯度”，我们构造一个巨大的向量（Augmented State）：
$$ \mathbf{Z}(t) = \begin{pmatrix} \mathbf{x}(t) \\ \mathbf{a}(t) \end{pmatrix} $$
其中 $\mathbf{x}(t)$ 是图像状态，$\mathbf{a}(t) = \frac{\partial L}{\partial \mathbf{x}(t)}$ 是我们要算的梯度（Adjoint state）。

#### 理解 Eq. (6) (The Augmented SDE)
这个公式描述了如何通过解一个新的 SDE，从 $t=0$（净化结束时刻）反推回 $t=t^*$（净化开始时刻），从而得到关于输入 $x(t^*)$ 的梯度。

$$ \begin{pmatrix} \mathbf{x}(t^*) \\ \frac{\partial L}{\partial \mathbf{x}(t^*)} \end{pmatrix} = \text{sdeint}\left( \begin{pmatrix} \hat{\mathbf{x}}(0) \\ \frac{\partial L}{\partial \hat{\mathbf{x}}(0)} \end{pmatrix}, \tilde{\mathbf{f}}, \tilde{\mathbf{g}}, \tilde{\mathbf{w}}, 0, t^* \right) $$

**逐项解析：**
1.  **初始条件（Input）**：
    *   $\hat{\mathbf{x}}(0)$：这是前向传播（净化过程）算出来的最终干净图。
    *   $\frac{\partial L}{\partial \hat{\mathbf{x}}(0)}$：这是 Loss 对干净图的梯度（通过普通 BP 从分类器传回来的）。
2.  **积分方向**：从 $0$ 积到 $t^*$。注意这和净化过程的时间流向是相反的！我们在“倒带”。
3.  **最终结果（Output）**：
    *   上半部分 $\mathbf{x}(t^*)$：因为我们倒带了，所以我们顺便**重构**了输入时的状态（这就是为什么不需要存中间状态，因为我们重新算了一遍）。
    *   下半部分 $\frac{\partial L}{\partial \mathbf{x}(t^*)}$：这就是你攻击所需的**核心梯度**！

---

### 3. 最难懂的部分：增广漂移项 (Augmented Drift) $\tilde{\mathbf{f}}$

在 Proposition 3.3 中，定义了新的漂移函数 $\tilde{\mathbf{f}}$：

$$ \tilde{\mathbf{f}}([\mathbf{x}; \mathbf{z}], t) = \begin{pmatrix} \mathbf{f}_{rev}(\mathbf{x}, t) \\ - \frac{\partial \mathbf{f}_{rev}(\mathbf{x}, t)}{\partial \mathbf{x}}^T \mathbf{z} \end{pmatrix} $$
*(注：原文写得稍微有点不一样，这里为了方便你理解物理意义，用了更通用的伴随方程形式，原文 Eq. 6 下方的定义是针对其特定时间变换后的形式)*

让我们看原文的具体定义：
$$ \tilde{\mathbf{f}} = \begin{pmatrix} \mathbf{f}_{rev}(\mathbf{x}, t) \\ \frac{\partial \mathbf{f}_{rev}(\mathbf{x}, t)}{\partial \mathbf{x}} \mathbf{z} \end{pmatrix} $$
这里 $\mathbf{z}$ 就是梯度向量。

**数学解读（Jacobian-Vector Product, JVP）：**
*   下半部分 $\frac{\partial \mathbf{f}_{rev}}{\partial \mathbf{x}} \mathbf{z}$ 是核心。
*   它计算的是原 SDE 的漂移函数 $\mathbf{f}_{rev}$ 对输入 $\mathbf{x}$ 的**雅可比矩阵 (Jacobian)** 与当前梯度向量 $\mathbf{z}$ 的乘积。
*   根据链式法则（Chain Rule），这就是梯度在连续时间系统中的传播方式。
*   **代码实操**：在 PyTorch 中，这不需要手写 Jacobian 矩阵（太大了），而是直接用 `torch.autograd.grad` 计算 Vector-Jacobian Product (VJP)。

---

### 4. 噪声的处理 (The Wiener Process $\tilde{\mathbf{w}}$)

细心的你会发现，SDE 是随机的。如果在算梯度的时候（倒带），走的路径和前向（净化）时的随机路径不一样，算出来的梯度就是错的。

*   **DiffPure 的处理**：原文提到 $\tilde{\mathbf{w}}(t) = (-\mathbf{w}(1-t), -\mathbf{w}(1-t))$。
*   **含义**：我们在解这个增广 SDE 时，必须**重用**前向传播时采样的同一个布朗运动噪声 $\mathbf{w}$。
*   **代码实现**：`torchsde` 库允许你传入一个固定的 `BrownianTree` 或使用相同的随机种子，确保 `sdeint` 和 `sdeint_adjoint` 走的是同一条随机轨迹。

---

### 5. 整个梯度计算的全流程总结 (Attack Pipeline)

作为一个攻击者，当你调用 `.backward()` 时，DiffPure 内部发生了以下事情：

1.  **Phase 1: 攻击梯度传入**
    *   分类器算出 Loss，梯度回传到 $\hat{\mathbf{x}}(0)$。
2.  **Phase 2: Adjoint SDE 启动 (Eq. 6)**
    *   构造增广向量 $[\hat{\mathbf{x}}(0); \nabla_{\hat{\mathbf{x}}} L]$。
    *   调用 Solver 从 $t=0$ 跑到 $t=t^*$。
    *   Solver 内部不断计算 $\mathbf{f}_{rev}$ 和它的 VJP。
3.  **Phase 3: 得到扩散后的梯度**
    *   Solver 结束，提取出下半部分向量，这就得到了 $\nabla_{\mathbf{x}(t^*)} L$。
4.  **Phase 4: 穿过前向扩散 (Forward Diffusion)**
    *   现在还要穿过那个“一步加噪”的过程（Eq. 3）。
    *   由于 $\mathbf{x}(t^*) = \sqrt{\alpha}\mathbf{x}_a + \dots$，这是线性的。
    *   直接用链式法则：$\nabla_{\mathbf{x}_a} L = \nabla_{\mathbf{x}(t^*)} L \cdot \sqrt{\alpha(t^*)}$。
5.  **Phase 5: 得到最终梯度**
    *   拿着 $\nabla_{\mathbf{x}_a} L$，你就可以做 PGD 的一步更新：$x_{adv} = x_{adv} + \epsilon \cdot \text{sign}(\nabla_{\mathbf{x}_a} L)$。

### 对你的科研 Tips

1.  **数值误差的坑**：
    *   Adjoint Method 虽然省显存，但以此换来的是**算力**（要多跑一遍 SDE）和**数值稳定性**风险。
    *   如果 SDE Solver 的步长（step size）太大，重构出来的 $\mathbf{x}(t^*)$ 会和原始的 $\mathbf{x}(t^*)$ 有偏差，导致梯度算不准。
    *   **Appendix B.5** 专门讨论了这个问题，你看图 5 (Figure 5)，步长越小误差越小。做实验时如果发现攻击效果奇怪，检查一下 step size。

2.  **Code 对应**：
    *   当你在 L20 上复现时，你会直接使用 `torchsde.sdeint_adjoint`。
    *   一定要看懂这个函数的参数，它直接封装了 Eq. 6 的所有逻辑。你只需要定义好 `drift` 和 `diffusion` 函数即可。

这就是 DiffPure 梯度计算的全部奥秘。理解了这套逻辑，你也就理解了为什么它能被攻击（因为梯度是可计算的，不是 Obfuscated），以及为什么它比单纯的 Randomized Smoothing 更难处理（因为这套梯度计算非常昂贵且复杂）。