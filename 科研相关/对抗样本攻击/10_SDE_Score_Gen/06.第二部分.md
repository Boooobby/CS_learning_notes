这一章（**Background**）看似是在 "炒冷饭"（回顾 SMLD 和 DDPM），但对于你来说，这是理解全篇论文 **"统一场论"** 的关键铺垫。

既然你已经读过 DDPM 和 DDIM，我不需要教你基础原理。我将**从“对比”的角度**为你解读这一章，并指出**这些区别对 DiffPure 和对抗防御研究的潜在影响**。

---

### 1. 两个门派的对决

在 SDE 统一江湖之前，生成模型分为两个流派。作者在这里把公式列出来，是为了让读者看到它们惊人的相似性和关键的差异。

#### 派系 A：SMLD (Denoising Score Matching with Langevin Dynamics)
*   **代表作**：Song & Ermon (NeurIPS 2019), NCSN。
*   **加噪方式 (Perturbation Kernel)**：
    *   $p_\sigma(\tilde{x}|x) = \mathcal{N}(\tilde{x}; x, \sigma_i^2 I)$
    *   **特点**：直接在原图上叠加高斯噪声，**不缩放原图**。
    *   **后果**：随着噪声变大，总方差 $\sigma_i^2$ 会越来越大，最终趋于爆炸。这就是后文 **VE-SDE (Variance Exploding)** 的由来。
*   **采样方式 (Eq. 2)**：
    *   使用 **Langevin MCMC**。在一个噪声等级 $\sigma_i$ 上，通过多次迭代（$M$ 步）把样本推通过梯度推向高概率区域。
    *   **公式**：$x_{m} = x_{m-1} + \epsilon s_\theta(...) + \text{noise}$。这是一个迭代修正的过程。

#### 派系 B：DDPM (Denoising Diffusion Probabilistic Models)
*   **代表作**：Ho et al. (NeurIPS 2020)。
*   **加噪方式**：
    *   $x_i = \sqrt{1-\beta_i}x_{i-1} + \sqrt{\beta_i}z$
    *   **特点**：在加噪的同时，**把原图乘了一个小于1的系数**。
    *   **后果**：无论怎么加噪，只要控制好 $\beta$，最终的数据方差可以保持在 1 (Standard Gaussian)。这就是后文 **VP-SDE (Variance Preserving)** 的由来。
*   **采样方式 (Eq. 4)**：
    *   **Ancestral Sampling**（祖先采样）。显式地去预测 $x_{i-1}$ 的分布均值。
    *   这其实是将逆向过程看作一个马尔可夫链。

---

### 2. 作者的 "神来之笔"：殊途同归

这一章最重要的一句话在 **2.2 节末尾**：
> "The objective Eq. (3) described here [DDPM]... matches the score of the perturbed data distribution... related to corresponding perturbation kernels in the same functional form."

**解读**：
作者指出，DDPM 那个看起来很复杂的 $L_{simple}$ 损失函数，本质上**也是在做 Score Matching**！
*   SMLD 在学：$\nabla_x \log p_{\sigma}(x)$
*   DDPM 在学：$\nabla_x \log p_{\alpha}(x)$ (只是前面多了些权重系数)

**结论**：既然大家都在学 "Score"（梯度场），那底层的数学逻辑应该是一样的。这就引出了下一章的 SDE。

---

### 3. 给你的科研 "Idea" (对抗样本/DiffPure 视角)

当你读这一章时，请思考以下 3 个与 DiffPure 紧密相关的问题：

#### Idea 1: 采样器的鲁棒性差异 (SMLD vs DDPM)
*   **思考**：目前 DiffPure 大多基于 DDPM 的采样方式（预测均值）。
*   **区别**：
    *   SMLD 的采样 (Eq. 2) 是纯粹基于 **Langevin Dynamics** 的，它是利用梯度场慢慢 "爬" 向数据流形。
    *   DDPM 的采样 (Eq. 4) 是预测每一步的去噪结果。
*   **Hypothesis**：**Langevin Dynamics (SMLD方式) 会不会对对抗攻击更鲁棒？**
    *   因为 Langevin Dynamics 本质上是一种 MCMC，它并不在乎你当前在哪，只在乎梯度指向哪，且带有强随机性。如果攻击者通过微小扰动骗过了 DDPM 的均值预测，可能骗不过 Langevin 的梯度修正。这值得做一个对比实验。

#### Idea 2: 缩放的影响 (Scaling Issue)
*   **DDPM (DiffPure常用)**：输入 $x_{adv}$ 进网络前，会被缩放（乘 $\sqrt{\bar{\alpha}_t}$）。这意味着攻击的 $\epsilon$ 也会被“缩小”。
*   **SMLD**：输入就是 $x_{adv} + \text{Noise}$，没有缩放。
*   **思考**：对于大范数攻击（如 $\ell_\infty = 16/255$），DDPM 的缩放是不是间接地帮助了防御？还是说 SMLD 那种直接叠加巨大方差噪声的方式（Exploding Variance）能更好地覆盖住攻击？
    *   *注*：SMLD 的 $\sigma_{max}$ 通常极大（比如 50 或 90），远超像素范围，这可能对大扰动有奇效。

#### Idea 3: 这里的公式是 SDE 离散化的基础
*   你要记住 eq(2) 和 eq(4)。
*   到了第 4 章讲 **Predictor-Corrector** 时，Predictor 往往对应 eq(4) 的连续版，而 Corrector 对应的就是 eq(2) 的逻辑。
*   **DiffPure 改进点**：能不能把 eq(2) 强行插入到 DiffPure 的每一步里？做所谓的 "Purification with Correction"。

---

### 总结

读这一章，不要去纠结推导。
**只要记住：**
1.  **SMLD** = 不缩放原图 + 甚至不需要归一化 + 纯梯度引导。
2.  **DDPM** = 缩放原图 + 用 UNet 预测噪声 + 马尔可夫链还原。
3.  **DiffPure 现状** = 目前用的是 DDPM 这一套。
4.  **你的机会** = 试试把 SMLD 那一套拿过来做防御，或者把 SMLD 的 Langevin 步骤作为插件插进 DiffPure。