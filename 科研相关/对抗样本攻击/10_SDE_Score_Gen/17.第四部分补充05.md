讲讲4.3

---

来到 **4.3 Probability Flow ODE（概率流 ODE）**，我们进入了这篇论文最“科幻”的部分。

如果说 4.1 和 4.2 还是在处理**随机性**（SDE, Langevin Dynamics 都有随机噪声 $\mathbf{z}$），那么 4.3 则是把整个过程变成了一种**确定性**的数学变换。

这也解释了为什么 **DDIM** (Song et al., ICLR 2021) 能够奏效——因为 DDIM 其实就是这里提到的 ODE 的离散化版本。

---

### 1. 核心概念：给“醉汉”画出一条“醒酒路线”

我们来做一个直观的类比：

*   **SDE (随机微分方程)**：就像一个**醉汉**回家。他每一步都跌跌撞撞（加入随机噪声 $d\mathbf{w}$），但他只要大致方向是对的（Score Function 指引），无数次重复后，这些醉汉在某个时间点的分布统计规律是稳定的。
*   **Probability Flow ODE (概率流 ODE)**：数学家发现，对于上面那一群醉汉的分布，存在一种**完全清醒的走法**。如果我们让一个人严格按照某个特定的速度公式走，完全不加随机抖动，他在每一时刻所处的位置分布，竟然能和那群醉汉的分布**一模一样**。

**这就是本节的核弹级结论：**
对于任意一个扩散 SDE，都存在一个对应的确定性 ODE，它们的边缘概率分布 $p_t(x)$ 是完全相同的。

### 2. 数学方程：大家来找茬

让我们对比一下 SDE 和 ODE 的公式（注意系数）：

*   **SDE (4.1节)**:
    $$ d\mathbf{x} = [\mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x})] dt + \underbrace{g(t) d\mathbf{w}}_{\text{有随机项}} $$
*   **ODE (4.3节)**:
    $$ d\mathbf{x} = [\mathbf{f}(\mathbf{x}, t) - \frac{1}{2}g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x})] dt \quad \underbrace{\text{(无随机项)}}_{\text{也就是} dw=0} $$

**注意那个 $\frac{1}{2}$！**
为什么是 $1/2$？这是通过 **Fokker-Planck 方程** 推导出来的（Paper Appendix D.1 有详细证明）。简单来说，为了让确定性的轨迹能模拟出随机扩散出来的那个“扩散开”的分布形状，确定性的漂移力度必须调整（正好减半），才能在数学上严丝合缝地对齐。

### 3. 这个 ODE 带来了什么？（三大神器）

既然变得“确定”了，我们就获得了很多随机过程中没有的超能力：

#### 神器 A：精确的似然计算 (Exact Likelihood Computation)
这是 Diffusion Model 以前做不到的。
*   **原理**：利用 **Neural ODE** 中的“瞬时变量代换公式”（Instantaneous Change of Variables）。
*   **操作**：我们可以把一张图片 $x(0)$ 乃至它的概率密度，沿着 ODE 积分推到 $x(T)$（高斯分布）。因为高斯分布的概率我们是知道的，而且积分过程的可逆变换我们也知道，所以我们竟然可以**精确算出一张图片的 $p(x)$（即 NLL, Negative Log-Likelihood）**。
*   **意义**：我们可以知道模型认为这张图“有多真”。

#### 神器 B：可逆编码与潜空间操作 (Uniquely Identifiable Encoding)
*   **原理**：ODE 是确定性的双射（Bijection）。
*   **操作**：给定一张图 $x(0)$，通过 ODE 积分到 $t=T$，你会得到一个**唯一确定的**噪声向量 $z_T$。反之亦然。
*   **意义**：这让 Diffusion 变得像 GAN 或 Flow 模型一样，可以对 Latent Code 进行插值、修改，实现属性编辑。

#### 神器 C：黑盒快速求解 (Fast Black-Box Sampling)
*   **原理**：既然它是 ODE，我们就可以祭出数学界几百年积累的 **ODE Solvers**（比如 `Runge-Kutta 45`，`Dormand-Prince`）。
*   **操作**：这些 Solver 自带“自适应步长”功能。路平的时候步子迈大点，路陡的时候步子迈小点。
*   **意义**：这可以在保证误差可控的前提下，极大地减少采样步数（比 DDIM 还快），大幅提升推理速度。

---

### 4. 给 DiffPure 的科研显微镜 (Research Ideas)

这是你最关心的部分。ODE 视角能给对抗防御带来什么？

#### 💡 Idea 1: 对抗样本检测器 (The Likelihood Detective)
*   **洞察**：对抗样本 $x_{adv}$ 的一个显著特征是它们通常位于数据流形的**低概率密度区域**（Off-manifold）。
*   **痛点**：以前我们很难度量这个“概率密度”到底是多少。
*   **Idea**：在 DiffPure 之前加一个检测挂件。利用 ODE 计算输入图像的 **NLL (Negative Log-Likelihood)**。
    *   如果 $NLL(x) > \text{Threshold}$，直接报警说是对抗样本。
    *   或者，把 NLL 作为一个损失函数，指导梯度的优化方向，主动把样本拉回高 NLL 区域？

#### 💡 Idea 2: 确定性防御 vs. 梯度掩盖 (Deterministic Defense & Gradient Masking)
*   **洞察**：DiffPure 的原论文主要依赖 SDE 的随机性来防御（Stochasticity is a defense）。攻击者很难计算随机过程的期望梯度（EOT 攻击成本高）。
*   **Idea**：如果你把 DiffPure 的内核换成 **ODE Solver**（变成确定性防御）：
    *   **坏处**：攻击者可能更容易计算梯度（因为过程是可微且确定的 Neural ODE），可能更容易被白盒攻击击穿。
    *   **好处**：防御速度会快非常多（RK45 几十步就能搞定）。
    *   **实验**：对比 SDE-DiffPure 和 ODE-DiffPure。研究**随机性**在对抗防御中究竟贡献了多少 Robustness？这在机理研究上非常有价值。

#### 💡 Idea 3: 可逆纯化 (Invertible Purification)
*   **洞察**：DiffPure 的标准做法是：$x \to x_{noisy} \to x_{rec}$。这是一个有损过程。
*   **Idea**：利用 ODE 的可逆性。
    *   **Encode**: $x_{adv} \xrightarrow{\text{ODE Forward}} z_{adv}$ (得到在 $T$ 时刻的潜变量)。
    *   **Manifold Projection**: 我们假设对抗扰动在高维 Latent Space 可能表现为异常值。能不能在 $z$ 空间做简单的截断或去噪？
    *   **Decode**: $z_{cleaned} \xrightarrow{\text{ODE Backward}} x_{purified}$。
    *   这有点像基于 Flow 的防御，但你是用目前最强的 Diffusion Backbone 做的。

### 总结
Section 4.3 把原本云雾缭绕的 Diffusion 变成了清晰精确的 ODE 轨迹。
对于你的 DiffPure 研究，最大的两把武器是：
1.  **Likelihood**（用来做检测/判别）。
2.  **Efficiency**（用 ODE Solver 加速防御推理）。

至此，你已经掌握了 Section 4 的三把斧头：
*   **4.1**: SDE 怎么解（基础）。
*   **4.2**: 怎么解得更好（PC Sampler，适合增强防御）。
*   **4.3**: 怎么确定性地解（ODE，适合检测和加速）。

接下来如果还有精力，**Section 5 (Controllable Generation)** 会教你怎么在生成过程中**加条件**（比如 Class-condition），这对做 **Guided-DiffPure**（利用标签信息引导去噪）非常有用！