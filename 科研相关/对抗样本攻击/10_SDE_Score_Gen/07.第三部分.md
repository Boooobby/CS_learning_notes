好的！**Section 3** 是这篇论文的灵魂，也是 **DiffPure 和所有基于Diffusion的对抗防御背后的真正物理图景**。

不理解这一章，你做 DiffPure 就只是在“调包”；理解了这一章，你才能明白**为什么 DiffPure 能净化对抗样本**，以及**如何从数学上改进它**。

我会沿着作者的**数学逻辑链条**（从正向到逆向，从离散到连续），拆解每一个环节，并随时插入**对抗样本视角**的注解。

---

### 第一环：从离散走向连续 (The Limit)

#### 1. 正向过程 (Forward SDE)
**核心思想**：我们不再看离散的 $x_0, x_1, ..., x_N$，而是看一个随时间 $t \in [0, T]$ 演变的随机过程 $\{x(t)\}$。

**公式 (5)**：
$$ dx = f(x, t)dt + g(t)dw $$

*   **$x$**：你的图片（或者对抗样本 $x_{adv}$）。
*   **$f(x, t)$**：**漂移系数 (Drift)**。它决定了数据“平均”往哪里走。
    *   在 DiffPure (基于 DDPM) 中，这个项通常是 $-\frac{1}{2}\beta(t)x$。意思是把这堆像素值慢慢拉向 0（均值回归）。
*   **$g(t)$**：**扩散系数 (Diffusion)**。它决定了有多少噪声（随机性）注入进来。
*   **$dw$**：**维纳过程 (Wiener Process)**，也就是布朗运动。这是随机性的来源。

> **逻辑链条点拨**：
> 这是一个 IT$\hat{O}$（伊藤）积分方程的微分形式。它的物理意义是：在一个极短的时间 $dt$ 内，图片的变化量包含两部分：一部分是确定性的衰变（Drift），另一部分是纯粹的随机抖动（Diffusion）。
>
> **对抗防御视角**：
> 对抗扰动 $\delta$ 是一种精心设计的、微小的、针对特定模型梯度方向的扰动。
> 当你把 $x_{adv} = x + \delta$ 放入这个方程：
> 1.  $f(x,t)$ 会把 $\delta$ 和 $x$ 一起缩小（Signal Decay）。
> 2.  **更重要的是 $g(t)dw$**：这个随机抖动是全方位的、各向同性的。对于脆弱的高频对抗扰动 $\delta$，这种连续不断的随机“轰炸”极易破坏其精细结构。这就是 Diffusion 能防御的核心物理机制——**Brownian motion destroys adversarial structure.**

#### 2. 两个具体的例子 (VE & VP)
这一章最重要的数学推导在 **Appendix B**，但在正文中总结为了两个结论：

*   **VE-SDE (对应 SMLD)**：
    $$ dx = \sqrt{\frac{d[\sigma^2(t)]}{dt}} dw $$
    *   **特点**：漂移项 $f=0$。图片不衰减，只是疯狂加噪声。方差爆炸。
*   **VP-SDE (对应 DDPM, DiffPure常用)**：
    $$ dx = -\frac{1}{2}\beta(t)x dt + \sqrt{\beta(t)} dw $$
    *   **特点**：有漂移项。方差保持恒定。

---

### 第二环：时间的逆流 (Reverse SDE)

这是整篇论文最精彩的数学魔法，引用了安德森 (Anderson, 1982) 的定理。

**逻辑问题**：正是有了正向 SDE 把数据变成了噪声（先验 $p_T$），如果我们能把这个 SDE 倒过来走，就能把噪声变回数据。但是，SDE 里面含随机项 $dw$，这玩意儿怎么倒？

**公式 (6) - 逆向 SDE**：
$$ dx = [f(x, t) - g(t)^2 \nabla_x \log p_t(x)] dt + g(t) d\bar{w} $$

*   **$dt$**：这里代表时间倒流的负时间步。
*   **$d\bar{w}$**：这是逆向时间流里的布朗运动（依然是随机的！）。
*   **$- g(t)^2 \nabla_x \log p_t(x)$**：**这是关键项！**
    *   正向时，$f(x,t)$ 负责把数据拉向 0 (或别的先验)。
    *   逆向时，这一项负责**对抗**漂移和扩散，把跑偏的 $x$ 强行拉回数据分布 $p_t(x)$ 的高概率区域。
    *   $\nabla_x \log p_t(x)$ 就是 **Score Function**。

> **对抗防御的底层逻辑 (The Defense Mechanism)**：
> 1.  **Forward**: 你把对抗样本 $x_{adv}$ 丢进正向 SDE。假设走到 $t=0.5$，此时 $x(0.5)$ 充满了噪声，且对抗扰动 $\delta$ 已经被 $dw$ 破坏得差不多了。
> 2.  **Reverse**: 开始运行公式 (6)。
>     *   此时，网络计算的是 Score $\nabla_x \log p_t(x)$。注意，$p_t(x)$ 是**针对Clean Data训练的分布**。
>     *   所以，Score 指向的是**最接近当前含噪图像的“真实流形”上的点**，而不是指向那个被攻击后的错误类别流形。
>     *   这就是**净化 (Purification)** 的数学本质：利用 Score Field 的梯度场，把 stray point 重新投影回 clean manifold。

---

### 第三环：==如何求 Score？==(Score Matching)

既然逆向过程全指望 $\nabla_x \log p_t(x)$，那这个东西怎么求？我们只有样本，没法直接求分布的梯度。

**公式 (7)**：
$$ \theta^* = \arg \min_\theta \mathbb{E} [ || s_\theta(x(t), t) - \nabla_{x(t)} \log p_{0t}(x(t)|x(0)) ||^2 ] $$

*   **逻辑转换**：计算整个分布的边缘 Score $\nabla_x \log p_t(x)$ 很难。但是，计算**条件分布**的 Score $\nabla_x \log p_{0t}(x(t)|x(0))$ 很简单！
    *   因为从 $x(0)$ 到 $x(t)$ 的转移核 $p_{0t}$ 通常是高斯分布 $\mathcal{N}(x(t); \mu, \Sigma)$。
    *   高斯的 Score 就是 $-(x(t) - \mu) / \Sigma$。闭式解！
*   **结论**：只要训练神经网络去预测“加进去的那个噪声”，实际上就是在学习 Score。这统一了 DDPM 的 loss 和 Score Matching。

---

### 你的“Takeaway” (重点笔记)

读完第三部分，你的脑子里要建立起这样一个**动态防御图景**：

1.  **对抗样本 $x_{adv}$ 是流形外的一个点**。
2.  **Forward SDE** 是一场“沙尘暴” ($dw$)，把这个点吹得更远，同时也吹散了点上的特殊纹理（对抗扰动）。
3.  **Reverse SDE** 是一根由 Score Function 控制的“弹力绳”。
4.  因为神经网络是在干净数据上训练的，所以这根绳子的锚点永远在**真实流形**上。
5.  当我们启动 DiffPure (Reverse SDE) 时，这根绳子就把迷失在沙尘暴里的点，强行拉回到了**真实流形**，而不是拉回到攻击者希望的那个错误位置。

**建议复现动作**：
在你看代码时，去找那个 `sde.reverse(score_fn)` 的函数。你会看到它精确地实现了公式 (6)。尝试修改里面的系数，看看对净化效果有什么影响。