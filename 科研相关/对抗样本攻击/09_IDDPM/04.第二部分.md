**Section 2 (Denoising Diffusion Probabilistic Models)** 实际上是对 DDPM 原作（Ho et al., 2020）的一个**快速回顾与复盘**。

既然你已经读过 DDPM 原作，这一章对你来说应该是“熟悉的配方”。但作者在这里并不是在凑字数，他在**为 Section 3 的改进埋雷（伏笔）**。他把 DDPM 的数学框架摆出来，是为了精准地指出其中那个**被忽略的缺陷**。

我把你**必须彻底搞懂**的公式和逻辑拆解如下（这也会对应你复现 IDDPM 代码的基础部分）：

---

### 1. 核心定义：扩散模型的 VAE 本质 (2.1 Definitions)

作者首先明确了扩散模型其实就是一个 **固定编码器 + 可学习解码器** 的 VAE。

*   **前向过程 (Q)：** 是一个固定的马尔可夫链，不断加高斯噪声。
    *   **关键公式 (Eq. 8)：** **“任意时刻直达公式”**
        $$ q(x_t|x_0) = N(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I) $$
        *   **复现/DiffPure 重点：** 这个公式是扩散模型的灵魂。你在做 DiffPure 时，攻击者把对抗样本 $x_{adv}$ 喂进来，你需要把它加噪到 $t$ 时刻，用的就是这个公式（而不是写一个 for 循环循环 $t$ 次）。这是一步到位的。

*   **逆向过程 (P)：** 我们要训练神经网络 $p_\theta$ 去拟合真实的逆向分布。
    *   **关键公式 (Eq. 3)：**
        $$ p_\theta(x_{t-1}|x_t) := N(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) $$
        *   **伏笔来了：** 这里有两个参数：**均值 $\mu_\theta$** 和 **方差 $\Sigma_\theta$**。
        *   DDPM 原作花了很大力气研究 $\mu_\theta$ 怎么预测，但对于 $\Sigma_\theta$ 竟然选择了**“直接固定”**（即不通过网络学习）。

### 2. 真实后验分布：上帝视角 (The True Posterior)

如果我们知道 $x_0$（也就是知道原图长什么样），那么从 $x_t$ 倒推 $x_{t-1}$ 是有解析解（Closed-form）的。这个分布叫 **真实后验 $q(x_{t-1}|x_t, x_0)$**。

*   **关键公式 (Eq. 9, 10, 11)：**
    *   **均值 $\tilde{\mu}_t$ (Eq. 10)：** 这是网络 $\mu_\theta$ 努力要去逼近的目标。它是 $x_t$ 和 $x_0$ 的加权平均。
    *   **方差 $\tilde{\beta}_t$ (Eq. 9)：**
        $$ \tilde{\beta}_t := \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t $$
        这个公式极为重要！它是**真实后验分布的方差**。IDDPM 的 Section 3 整个就是围绕这个变量展开的。

### 3. DDPM 的“偷懒”与训练目标 (2.2 Training in Practice)

这是本章中最关键的一点，指出了 IDDPM 要攻击的靶子。

*   **Loss 函数：**
    *   理论上应该优化 **VLB (Variational Lower Bound, Eq. 4)**。
    *   但实操中，DDPM 发现忽略具体的方差项，直接预测噪声 $\epsilon$，并使用 **$L_{simple}$ (Eq. 13)** 效果更好：
        $$ L_{simple} = ||\epsilon - \epsilon_\theta(x_t, t)||^2 $$
    *   **复现重点：** 这个 Loss 只监督了均值（因为它让网络预测噪声，其实等价于预测均值），**完全没有监督方差**。

*   **关于方差的争议（重点中的重点）：**
    *   DDPM 原作中，因为 $L_{simple}$ 不学方差，所以作者被迫把方差 $\Sigma_\theta$ 设为常数。
    *   当时有两个选择：
        1.  设为 **$\beta_t$**（上限，假设初始就是完全各向同性噪声）。
        2.  设为 **$\tilde{\beta}_t$**（下限，假设初始点是确定的）。
    *   **伏笔（最后一段）：** Ho et al. 发现这两者选哪个**画质都差不多**，所以就没细究。
    *   **IDDPM 的反驳：** 这两者只是在 $T=1000$ 这种精细步数下“差不多”。一旦步数变少，或者看 Log-likelihood 指标，这也就是 IDDPM 为什么要提出 **Learning $\Sigma_\theta$** 的原因。

---

### 对你做 DiffPure 的启示 (Section 2 版)

虽然这章是 Review，但在从代码复现和 DiffPure 的角度，你要看懂以下变量在代码里的名字（通常在 `gaussian_diffusion.py` 里）：

1.  **`posterior_variance` ($\tilde{\beta}_t$)**：
    在代码初始化 schedule 时，这个会被预计算出来。IDDPM 后来预测的方差就是在这个 `posterior_variance` 和 `betas` 之间做插值。如果不理解 Eq. 9，你就看不懂代码里那个插值计算。

2.  **DiffPure 的过程映射**：
    *   **攻击（Attack）：** 生成 $x_{adv}$。
    *   **净化前向 (Forward)：** 用 **Eq. 8** 把 $x_{adv}$ 变成 $x_t$。
    *   **净化后向 (Reverse)：** 用 **Eq. 3**（IDDPM 版本）把 $x_t$ 一步步降噪回 $\hat{x}_0$。

**总结 Section 2：**
作者在这里立好了靶子——**“DDPM 居然不学习方差，而且用了一个虽然好用但理论简化的 MSE Loss”**。接下来的 Section 3，作者就要把这两个坑填上，从而实现模型性能的飞跃。

准备好进入最精彩的 Section 3 吗？那是这篇论文数学密度最高、也是贡献最大的地方。