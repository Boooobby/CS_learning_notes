这一章（Section 3）是整篇论文的**灵魂**。如果不理解这里的逻辑链条，你在复现代码时就会对着那几行 Loss 计算和 `model_var_type` 发呆。

作者在这里的逻辑非常严密，是一环扣一环推导出来的。我将按照**“发现问题 -> 数学分析 -> 提出假设 -> 工程解决”**的链条为你拆解。

---

### 逻辑链条第一环：动机 (Motivation)
**为什么突然要在意“对数似然”(Log-Likelihood)？**

*   **现状：** DDPM 用简单的 MSE Loss ($L_{simple}$) 就能画出很好的图。
*   **问题：** 但是 DDPM 的 NLL (Negative Log-likelihood) 很差。
*   **数学直觉：** NLL 差，说明模型估计的**概率分布密度**不对。
    *   对于生成图片，也许只要均值（Mean）对了，图就好看了。
    *   但是！如果你想做**少步数采样**（DiffPure 的刚需），或者想做**防御**（覆盖真实分布），如果分布的“胖瘦”（方差）估计错了，你的采样路径就会偏离流形。
*   **结论：** 必须把 NLL 提上去。如果不把 NLL 提上去，DDPM 就只是一个画图玩具，而不是一个严谨的概率模型。

---

### 逻辑链条第二环：归因 (Attribution)
**由 NLL 差，想到了方差 $\Sigma_\theta$ 的问题。**

*   **回顾 ELBO 公式：**
    我们要优化的 NLL 上界是 $L_{vlb}$。如果你看 Eq. 6：
    $$ L_{t-1} = D_{KL}(q(x_{t-1}|x_t,x_0) || p_\theta(x_{t-1}|x_t)) $$
    这是一项 KL 散度。两个高斯分布的 KL 散度由什么决定？**既由均值差决定，也由方差比决定。**

*   **DDPM 的失误：**
    *   DDPM 甚至没有去学方差，直接把方差固定死了（$\sigma_t^2 = \beta_t$ 或 $\tilde{\beta}_t$）。
    *   **关键图表证据 (Figure 1)：**
        作者画图发现，当 $T=1000$ 时，$\beta_t$ 和 $\tilde{\beta}_t$ 确实差不多。但是！**当 $T$ 变小（例如 50 步）或者在 $t$ 接近 0 时，这两个值的差距是巨大的（对数坐标下差几个数量级）。**
*   **推论：** 固定方差是导致 NLL 差、且无法做少步数加速的罪魁祸首。
*   **决策：** **必须让神经网络学习方差 $\Sigma_\theta(x_t, t)$。**

---

### 逻辑链条第三环：建模 (Modeling)
**既然要学方差，网络该输出什么？**

这是工程上最精彩的一步。

*   **尝试 1（失败）：** 直接让网络输出一个值作为方差。
    *   *结果：* 很难训练，方差范围跨度太大，不稳定。
*   **数学边界思维：**
    我们知道真实的逆向方差肯定介于两个极端之间：
    1.  **上限 $\beta_t$**：对应完全不知道 $x_0$（纯各向同性扩散）。
    2.  **下限 $\tilde{\beta}_t$**：对应完全知道 $x_0$（确定性最强）。
    *   真实的方差肯定在这个区间里。
*   **解决方案 (Interpolation, Eq. 14)：**
    不要直接预测方差，而是预测**插值系数 $v$**。
    让网络输出一个 $v$（通过 Sigmoid 限制在 0~1 之间），然后在**对数域（Log domain）**做插值：
    $$ \Sigma_\theta(x_t, t) = \exp \left( v \cdot \log \beta_t + (1-v) \cdot \log \tilde{\beta}_t \right) $$
    *   **为什么在对数域？** 因为 $\beta$ 的数值非常小（$10^{-5}$ 级别），直接线性插值容易数值不稳定，对数域更符合神经网络处理量的习惯。

---

### 逻辑链条第四环：优化 (Optimization)
**有了新的输出 $v$，Loss 函数怎么改？**

这是最让人头的一步，也是你代码里的难点。

*   **冲突：**
    *   我们之前用的 $L_{simple}$ (MSE Loss) 效果极好，但它**只包含均值，不包含方差**。它对 $v$ 的梯度是 0。
    *   如果我们改用完整的 $L_{vlb}$（包含方差项），虽然理论正确，但实测画质会变差，且梯度极其不稳定。

*   **博弈与妥协：**
    我们需要一个集大成者——既要 $L_{simple}$ 的画质，又要 $L_{vlb}$ 对 $v$ 的指导。

*   **解决方案 (Hybrid Objective, Eq. 15)：**
    $$ L_{hybrid} = L_{simple} + \lambda L_{vlb} $$
    作者设 $\lambda = 0.001$。看似只是加权，其实有一个**极其重要的工程 Trick**：
    
    1.  **对于均值 $\mu_\theta$ 的学习：** 主要靠 $L_{simple}$。
    2.  **对于方差系数 $v$ 的学习：** 必须靠 $L_{vlb}$（因为 $L_{simple}$ 不管它）。
    3.  **Stop-Gradient（阻断梯度）：**
        在计算 $L_{vlb}$ 时，作者**切断了流向均值网络的梯度**。
        *   代码逻辑：`L_vlb(mu.detach(), var)`。
        *   **为什么？** 这种设计强迫 $L_{vlb}$ 这个 loss **只去更新方差头（输出 $v$ 的那部分参数）**，而不去干扰本来已经学得很好的均值部分。

---

### 逻辑链条第五环：修正 (Refinement)
**还要顺手修一个 Schedule 的 Bug。**

这是一个平行于方差的改进，但也为了提升 NLL。

*   **发现问题：**
    DDPM 用的 Linear Schedule（$\beta$ 线性增长）在 $64 \times 64$ 分辨率下有问题。
    *   看图 (Figure 3)：图还没怎么加噪呢，信号强度 $\bar{\alpha}_t$ 就掉光了。最后 20% 的步数纯粹在浪费算力处理纯噪声。
*   **数学解决：**
    换成 Cosine Schedule (Eq. 16)。
    *   让 $\bar{\alpha}_t$ 的下降像 $\cos^2$ 函数一样：两头平缓，中间陡峭。
    *   **收益：** 每一对 $(x_t, x_{t-1})$ 之间的差异更均匀，信息破坏更平滑。这对于 DiffPure 这种依赖中间状态 $x_t$ 的任务至关重要。

---

### 总结：Section 3 的完整逻辑闭环

1.  **目标：** 搞定 NLL，顺便搞定加速。
2.  **障碍：** 固定方差是瓶颈。
3.  **对策：** 学习方差 $\Sigma_\theta$。
4.  **手段：** 在 $\log \beta_t$ 和 $\log \tilde{\beta}_t$ 之间插值学习系数 $v$。
5.  **落地：** 用 $L_{hybrid}$ 训练，并且用 Stop-Gradient 让 NLL 专心修方差，MSE 专心修均值。

读懂了这个链条，你再去看代码里的 `losses.py`，看到 `vb_loss`（Variational Lower Bound loss）和 `mse_loss` 分开算再加起来，你就明白是为什么了。