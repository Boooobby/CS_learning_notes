好，既然你要做科研，咱们就得把“泛谈”收起来，直接上**推导**和**硬逻辑**。

这里是 DensePure **Section 3** 的深度解析，我们剥开代码的表象，看看数学到底是怎么推导出那个“并集凸集”结论的。

---

### 第一部分：核心推导基础 (Theorem 3.1)

**逻辑目标**：证明反向去噪过程生成的变量 $\hat{x}_0$，其条件概率密度函数 $P(\hat{x}_0| \hat{x}_t)$ 是由“数据先验”和“几何距离”共同决定的。

**数学推导过程**：

1.  **前提知识 (SDE与高斯加噪)**
    在扩散模型中，前向过程在时刻 $t$ 的边缘分布 $p_t(x_t)$ 可以看作是基于初始数据 $x_0$ 加了高斯噪声。
    如果我们已知 $x_0 = x$，那么 $t$ 时刻的分布是：
    $$ P(x_t = x_a | x_0 = x) = \mathcal{N}(x_a; \sqrt{\alpha_t}x, (1-\alpha_t)I) $$
    这里 $x_a$ 指的是对抗样本在 $t$ 时刻的状态（即 Scaling 后的对抗样本）。为了推导方便，忽略常数系数，其核心项是：
    $$ P(x_t = x_a | x_0 = x) \propto \exp\left( - \frac{\| \sqrt{\alpha_t}x - x_a \|^2}{2(1-\alpha_t)} \right) $$
    *论文简化处理*：论文中为了简化，把 Scale 放到了变量定义里，核心就是：**给定原图，生成噪声图的概率是一个高斯分布。**

2.  **贝叶斯逆转 (Bayes' Rule)**
    我们真正关心的是反向过程：**给定噪声图 $x_{a,t}$，它原本是 $x$ 的概率是多少？**
    应用贝叶斯公式：
    $$ P(\hat{x}_0 = x | \hat{x}_t = x_{a,t}) = \frac{P(\hat{x}_t = x_{a,t} | \hat{x}_0 = x) \cdot P(\hat{x}_0 = x)}{P(\hat{x}_t = x_{a,t})} $$

3.  **结果解析 (Theorem 3.1 公式)**
    $$ P(\hat{x}_0 = x | \hat{x}_t = x_{a,t}) \propto \underbrace{p(x)}_{\text{Prior}} \cdot \underbrace{\exp\left( - \frac{\|x - x_a\|^2}{2\sigma_t^2} \right)}_{\text{Likelihood}} $$
    
    *   **$p(x)$**: 这就是上面的 $P(\hat{x}_0 = x)$。即**数据流形密度**。如果 $x$ 是一张随机乱码图，这个值为0；如果是一张逼真的猫，$p(x)$ 很高。
    *   **$\exp(\dots)$**: 这就是上面的 Likelihood。即使 $x$ 是一张很完美的猫，如果它长得和输入的对抗样本 $x_a$ 哪怕有一点点像（距离很远），这一项也会迅速变为0。

**逻辑结论**：
DensePure 的净化过程，本质上是在寻找一个点 $x$，使得 **“它长得像真图”** ($p(x)$大) 且 **“它离对抗样本不远”** (高斯项大) 这两个条件同时满足乘积最大化。

---

### 第二部分：几何形状刻画 (Theorem 3.3 —— 全文最高能部分)

**逻辑目标**：既然我们知道了去噪是在找概率最大的点，那么**什么样的对抗样本 $x_a$ 能够被成功“拉回”正确的类别？** 这个安全区域的形状是什么？

作者在这里做了一个假设：**Deterministic Reverse**。即假设也就是我们取概率密度最大的那个 $x$ 作为输出：
$$ P(x_a; t) := \arg\max_x P(\hat{x}_0 = x | \hat{x}_t = x_{a,t}) $$

**硬核推导：为什么是凸集 (Convex Set)？**

考虑两个潜在的复原结果：
*   $x_0$：正确的原图（比如“猫”）。
*   $x'_0$：错误的图（比如“狗”）。

**问题**：在什么情况下，扩散模型会把 $x_a$ 恢复成 $x_0$ 而不是 $x'_0$？
**回答**：当 $x_0$ 的后验概率大于 $x'_0$ 时。

根据 Theorem 3.1，建立不等式：
$$ p(x_0) e^{-\frac{\|x_0 - x_a\|^2}{2\sigma^2}} > p(x'_0) e^{-\frac{\|x'_0 - x_a\|^2}{2\sigma^2}} $$

两边取自然对数 $ln$：
$$ \ln p(x_0) - \frac{\|x_0 - x_a\|^2}{2\sigma^2} > \ln p(x'_0) - \frac{\|x'_0 - x_a\|^2}{2\sigma^2} $$

展开 $L_2$ 范数 $\|x-y\|^2 = x^Tx - 2x^Ty + y^Ty$，并将含有 $x_a$ 的项移到一边。注意 $\|x_a\|^2$ 在两边会被消掉！
$$ \frac{1}{\sigma^2} (x_0^T x_a - {x'_0}^T x_a) > \text{Const}(x_0, x'_0) $$
$$ x_a^T (x_0 - x'_0) > \text{Const} $$

**几何意义**：
你看这个不等式 $x_a^T w > b$，其中 $w = x_0 - x'_0$。
这是一个关于变量 $x_a$ 的**线性不等式**。
在几何上，线性不等式代表什么？代表**半空间 (Half-space)**，边界是一个超平面。

**逻辑链条**：
1.  为了让 $x_a$ 被恢复成 $x_0$（正确），它在概率上必须胜过**任意一个**错误样本 $x'_0$。
2.  对于每一个错误样本 $x'_0$，都对应一个半空间限制。
3.  要胜过**所有**错误样本，就是这些半空间的**交集 (Intersection)**。
    $$ D_{sub}(x_0) = \bigcap_{x'_0} \text{HalfSpace}(x_0, x'_0) $$
4.  **数学性质**：半空间是凸集，凸集的交集依然是**凸集**。

**结论 1**：如果原图是**确定**的某一张 $x_0$，那么能被拉回这张图的对抗样本 $x_a$ 的区域，是一个凸集（多面体）。

**结论 2 (Union)**：
但是！标签为“猫”的图不只有 $x_0$ 这一张，还有 $\tilde{x}_0$（另一张猫的图）。
只要 $x_a$ 被拉回到 $x_0$ **或者** $\tilde{x}_0$，分类器都会判对。
所以，总的鲁棒区域（Robust Region）是：
$$ D_{total} = \bigcup_{\text{all clean cats}} D_{sub}(\text{cat}) $$
**多个凸集的并集，通常是非凸的，而且形状可能非常复杂、范围非常大。**

这就是为什么 DensePure 声称其鲁棒区域比 Cohen (2019) 的那个简单“球体”要优越得多的数学本质。

---

### 第三部分：理想与现实的差距 (Theorem 3.4)

**逻辑目标**：上面的推导都假设我们用的是完美的 $P_{reverse}$。但实际上我们是用神经网络（UNet）通过 Score Matching 训练出来的 $s_\theta(x, t)$。这个替换合法吗？误差多大？

**数学工具**：Girsanov Theorem（吉尔萨诺夫定理）。
这是一个随机分析中的定理，专门用来计算当 SDE 的漂移项（Drift term）发生变化时，概率测度（Measure）会怎么变。

*   **理想 SDE Drift**: $\nabla \log p_t(x)$
*   **实际 SDE Drift**: $s_\theta(x, t)$

**推导结果**：
两个分布之间的 KL 散度（差异）：
$$ D_{KL}(P_{ideal} || P_{model}) = \frac{1}{2} \int_0^t \mathbb{E} [ \| \nabla \log p_\tau(x) - s_\theta(x, \tau) \|^2 ] d\tau $$

**看右边这个积分是什么？**
$\| \nabla \log p - s_\theta \|^2$，这正好就是扩散模型的**训练损失函数 (Score Matching Loss)**！

**逻辑闭环**：
这证明了：只要我在训练扩散模型时 Loss 降得足够低，我的 DensePure 生成的条件分布 $P_{model}$ 就无限逼近于理论推导的 $P_{ideal}$。上面那些关于“并集凸集”的美好几何性质，在实际模型中就依然（近似）成立。

---

### 总结给你的 Takeaway

读这一节，不要只看公式，要看清楚作者是在玩什么**逻辑拼图**：

1.  用 **Bayes (Thm 3.1)** 建立去噪过程的概率解释：去噪 = 找先验和距离的平衡。
2.  用 **Linear Algebra (Thm 3.3)** 展开 $L_2$ 范数，利用消项技巧因为 $\|x_a\|^2$ 被消掉，证明边界是**线性**的，从而推导出**凸集**。
3.  用 **Union** 概念扩张地盘：真正的安全区是无数个凸集的叠加。
4.  用 **Girsanov (Thm 3.4)** 把数学理论和神经网络 Loss 挂钩，证明这套理论在工程上稍微有点误差也是可行的。

你在复现代码如果不明白为什么要多轮采样（Majority Vote），请回到 **Thm 3.1**：因为分布 $P$ 是存在的，单次采样只是从这个 $P$ 里摸一个点，多次采样才能逼近这个 $P$ 的 Mode（那个真正的 $x_0$）。